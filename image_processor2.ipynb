{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "QYPCvAY0v7Er",
        "outputId": "787fef56-87f2-4a53-cc3d-5165419efffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: urllib3 1.26.14\n",
            "Uninstalling urllib3-1.26.14:\n",
            "  Successfully uninstalled urllib3-1.26.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "Successfully installed urllib3-1.26.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <progress value='100' max=\"100\", style='width: 100%'>\n",
              "                100\n",
              "            </progress>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI2-THOR Version: 5.0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip uninstall urllib3 -y\n",
        "!pip install --upgrade urllib3\n",
        "!pip install --upgrade ai2thor ai2thor-colab &> /dev/null\n",
        "import ai2thor\n",
        "import ai2thor_colab\n",
        "\n",
        "from ai2thor.controller import Controller\n",
        "from ai2thor_colab import (\n",
        "    plot_frames,\n",
        "    show_objects_table,\n",
        "    side_by_side,\n",
        "    overlay,\n",
        "    show_video\n",
        ")\n",
        "\n",
        "ai2thor_colab.start_xserver()\n",
        "\n",
        "\"AI2-THOR Version: \" + ai2thor.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx-HdfjYwE9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a84b32-771b-4a46-c570-576456ad864c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.13.1+cu116)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (3.5.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.3.5)\n",
            "Collecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.22.4)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable_baselines3) (4.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (4.38.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (8.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines3) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.15.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616823 sha256=8c8d5e7afbead1b624a758ce191c4bc742a83b956598f40d1d08ea698faca02d\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "Successfully built gym\n",
            "Installing collected packages: importlib-metadata, gym, stable_baselines3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.21.0 importlib-metadata-4.13.0 stable_baselines3-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting img2vec_pytorch\n",
            "  Downloading img2vec_pytorch-1.0.1-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from img2vec_pytorch) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from img2vec_pytorch) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from img2vec_pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->img2vec_pytorch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->img2vec_pytorch) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->img2vec_pytorch) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->img2vec_pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->img2vec_pytorch) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->img2vec_pytorch) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->img2vec_pytorch) (1.26.14)\n",
            "Installing collected packages: img2vec_pytorch\n",
            "Successfully installed img2vec_pytorch-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sb3_contrib\n",
            "  Downloading sb3_contrib-1.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: stable-baselines3>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from sb3_contrib) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (4.13.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (1.13.1+cu116)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (3.5.3)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (0.21.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (2.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable-baselines3>=1.7.0->sb3_contrib) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3>=1.7.0->sb3_contrib) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable-baselines3>=1.7.0->sb3_contrib) (4.5.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (4.38.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3>=1.7.0->sb3_contrib) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3>=1.7.0->sb3_contrib) (1.15.0)\n",
            "Installing collected packages: sb3_contrib\n",
            "Successfully installed sb3_contrib-1.7.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install torch==1.12.0\n",
        "!pip install transformers\n",
        "#!pip install torchvision==0.13.0\n",
        "!pip install gym\n",
        "!pip install stable_baselines3\n",
        "!pip install einops\n",
        "!pip install img2vec_pytorch\n",
        "!pip install sb3_contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbJyblmUwciB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.resnet import resnet50, ResNet50_Weights\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1mvz4say3or"
      },
      "outputs": [],
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# CGAN Generator Model\n",
        "class FFTranslator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(FFTranslator, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(512, 768)\n",
        "        self.ln1 = nn.LayerNorm(768)\n",
        "        self.linear2 = nn.Linear(768, 1024)\n",
        "        self.ln2 = nn.LayerNorm(1024)\n",
        "        self.linear3 = nn.Linear(1024, 1536)\n",
        "        self.ln3 = nn.LayerNorm(1536)\n",
        "        self.linear4 = nn.Linear(1536, 2048)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        out = self.ln1(F.relu(self.linear1(input)))\n",
        "        out = self.ln2(F.relu(self.linear2(out)))\n",
        "        out = self.ln3(F.relu(self.linear3(out)))\n",
        "        out = torch.tanh(self.linear4(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEYJ90D4F9dY"
      },
      "outputs": [],
      "source": [
        "class GeneratorPreprocess(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(GeneratorPreprocess, self).__init__()\n",
        "          self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "          self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "          \n",
        "      @torch.no_grad()\n",
        "      def forward(self, text):\n",
        "          encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')#.to(device=torch.device(\"cuda:0\"))\n",
        "          out = self.model(**encoded_input)\n",
        "          out = mean_pooling(out, encoded_input[\"attention_mask\"])\n",
        "          out = F.normalize(out)\n",
        "          return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyyJyDb9xlLK"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        \n",
        "        self.translator = FFTranslator()\n",
        "\n",
        "        # hyperparams and train params\n",
        "\n",
        "    def forward(self, input):\n",
        "        noise = self.generate_noise()#.to(device=torch.device(\"cuda:0\"))\n",
        "        out = torch.cat((torch.squeeze(input), torch.squeeze(noise)), dim=0)\n",
        "        out = self.translator(out)\n",
        "        return out\n",
        "\n",
        "    def generate_noise(self):\n",
        "        return torch.normal(mean=torch.zeros(128), std=torch.zeros(128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-bkPnTVw5_0"
      },
      "outputs": [],
      "source": [
        "# embedder\n",
        "import stable_baselines3\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from torchvision import transforms\n",
        "import gym\n",
        "from einops import rearrange\n",
        "from PIL import Image\n",
        "from img2vec_pytorch import Img2Vec\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "    \n",
        "device = torch.device(dev)\n",
        "\n",
        "# model = model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "# feature_extractor = model\n",
        "\n",
        "class FeatureExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 512):\n",
        "        super(FeatureExtractor, self).__init__(observation_space, features_dim)\n",
        "        \n",
        "        # networks\n",
        "        self.generator = Generator()\n",
        "        self.generator.load_state_dict(torch.load('./generator (1).pt', map_location=\"cpu\"))\n",
        "        # self.feature_extractor = feature_extractor\n",
        "        # self.old_preprocess = weights.transforms()\n",
        "        # self.preprocess = transforms.Compose([\n",
        "        #     transforms.ToPILImage(),\n",
        "        #     transforms.Resize(256),\n",
        "        #     transforms.CenterCrop(224),\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "        # ])\n",
        "\n",
        "        \n",
        "        # load torch parameters for generator\n",
        "        \n",
        "        self.concat_processor = nn.Sequential(\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, features_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, obs_space: gym.spaces.Box):\n",
        "        image = torch.empty((obs_space.size()[0], 2048))\n",
        "        text = torch.empty((obs_space.size()[0], 384))\n",
        "        for ix in range(obs_space.size()[0]):\n",
        "            image[ix] = obs_space[ix,:-384,0]\n",
        "            text[ix] = obs_space[ix,2048:2048+384,0]\n",
        "        out = torch.empty(image.size(dim=0), 512)\n",
        "        for ix in range(image.size(dim=0)):\n",
        "          with torch.no_grad():\n",
        "              #filtered_input = rearrange(image[ix].squeeze(), 'h w c -> c h w')\n",
        "              #image_embed = self.preprocess(filtered_input)\n",
        "              # = torch.squeeze(self.feature_extractor(image_embed.unsqueeze(0)))\n",
        "              text_embed = torch.squeeze(self.generator(text[ix].squeeze()))\n",
        "          concat_embed = torch.cat((image[ix].squeeze(), F.normalize(text_embed, dim=0)), dim=0)\n",
        "          out[ix] = self.concat_processor(concat_embed)\n",
        "        return out\n",
        "\n",
        "def execute_resnet(image):\n",
        "  with torch.no_grad():\n",
        "    tensor = torch.Tensor(image.copy())\n",
        "    fe = Img2Vec(cuda=False, model='resnet50')\n",
        "    # preprocess = transforms.Compose([\n",
        "    #     transforms.RandomResizedCrop(224),\n",
        "    #     transforms.RandomHorizontalFlip(),\n",
        "    #     transforms.ToTensor(),\n",
        "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    # ])\n",
        "    tpi = transforms.ToPILImage()\n",
        "    reordered = tpi(rearrange(tensor, 'h w c -> c h w'))\n",
        "    image_embed = torch.squeeze(torch.from_numpy(fe.get_vec(reordered)))\n",
        "  return F.normalize(image_embed, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZeHH0M3xBvh"
      },
      "outputs": [],
      "source": [
        "kitchens = [f\"FloorPlan{i}\" for i in range(1, 21)]\n",
        "living_rooms = [f\"FloorPlan{200 + i}\" for i in range(1, 21)]\n",
        "bedrooms = [f\"FloorPlan{300 + i}\" for i in range(1, 21)]\n",
        "bathrooms = [f\"FloorPlan{400 + i}\" for i in range(1, 21)]\n",
        "\n",
        "trainMats = kitchens + living_rooms + bedrooms + bathrooms\n",
        "\n",
        "kitchens = [f\"FloorPlan{i}\" for i in range(21, 26)]\n",
        "living_rooms = [f\"FloorPlan{200 + i}\" for i in range(21, 26)]\n",
        "bedrooms = [f\"FloorPlan{300 + i}\" for i in range(21, 26)]\n",
        "bathrooms = [f\"FloorPlan{400 + i}\" for i in range(21, 26)]\n",
        "\n",
        "valMats = kitchens + living_rooms + bedrooms + bathrooms\n",
        "\n",
        "kitchens = [f\"FloorPlan{i}\" for i in range(26, 31)]\n",
        "living_rooms = [f\"FloorPlan{200 + i}\" for i in range(26, 31)]\n",
        "bedrooms = [f\"FloorPlan{300 + i}\" for i in range(26, 31)]\n",
        "bathrooms = [f\"FloorPlan{400 + i}\" for i in range(26, 31)]\n",
        "\n",
        "testMats = kitchens + living_rooms + bedrooms + bathrooms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZqZS4KsxCF5"
      },
      "outputs": [],
      "source": [
        "objects = {\n",
        "    \"AlarmClock\": \"An alarm clock is a small black box with two numbers separated by a colon on a digital display in the front.\",\n",
        "    \"AluminumFoil\": \"Aluminum foil is a shiny gray sheet that has a wrinkled metallic look.\",\n",
        "    \"Apple\": \"An apple is a red spherical shaped fruit with a small stem on top.\",\n",
        "    \"ArmChair\": \"An armchair is a large object with a lowered center to sit on and hightened sides and back to lay your arms and back on.\",\n",
        "    \"BaseballBat\": \"A baseball bat is a wooden elongated stick with a circular shape and a handle and widened part on opposite ends.\",\n",
        "    \"BasketBall\": \"A basketball is an orange color ball with bumps on it and black lines across it.\",\n",
        "    \"Bathtub\": \"A bathtub is a large white container with a metallic tap on one end to take baths in.\",\n",
        "    \"BathtubBasin\": \"A bathtub basin is a large white container with a metallic tap on one end to take baths in.\",\n",
        "    \"Bed\": \"A bed is a large white mat on top of a wooden frame that is used to sleep in.\",\n",
        "    \"Blinds\": \"Blinds are a set of thin wooden bars on windows that prevents you from seeing outside.\",\n",
        "    \"Book\": \"A book is a medium-sized rectangular item that has multiple pages when opened, and has a hard cover.\",\n",
        "    \"Boots\": \"Boots are a pair of medium-size shoes that are tall\",\n",
        "    \"Bowl\": \"A bowl is a wooden container with a circular bottom to hold items, usually food\",\n",
        "    \"Box\": \"A box is a cube-shaped brown item that can be used to store other items in when opened\",\n",
        "    \"Bread\": \"Bread is an edible item that is brown with a lighter inside, which usually has holes\",\n",
        "    \"ButterKnife\": \"A butter knife is a thin metallic object with a sharp edge used to cut butter when picked up\",\n",
        "    \"Cabinet\": \"A cabinet is a brown, wooden container connected to the wall that can store items when opened\",\n",
        "    \"Candle\": \"A candle is a small white stick of wax that can be lit with fire to provide light\",\n",
        "    \"CD\": \"A CD is a shiny, circular object with a hole in the middle used to play videos\",\n",
        "    \"CellPhone\": \"A cellphone is a small rectangular object with a translucent screen on the front and buttons on the sides\",\n",
        "    \"Chair\": \"A chair is an L-shaped wooden item that can be sat on, but does not have raised arm sides\",\n",
        "    \"Cloth\": \"A cloth is a flexible item found on tables that can be used to clean up areas\",\n",
        "    \"CoffeeMachine\": \"A coffee machine is a black mechanism with a stand to put a cup on where a dispenser is\",\n",
        "    \"CounterTop\": \"A countertop is a white flat surface elevated off the ground that is thick and patterned with abstract black spots.\",\n",
        "    \"CreditCard\": \"A credit card is a small, thin rectangular object with rounded corners and text on it\",\n",
        "    \"Cup\": \"A cup is a small cylinder with a hollow top to fill with items or liquids\",\n",
        "    \"Curtains\": \"Curtains are hanging cloths usually found on windows to cover from what is outside it\",\n",
        "    \"Desk\": \"A desk is a short table with a wooden top that usually comes with a chair\",\n",
        "    \"DeskLamp\": \"A desk lamp is a source of light that can be angled and placed on a desk\",\n",
        "    \"Desktop\": \"A desktop consists of a large metal box and a corresponding flat screen only a few feet wide\",\n",
        "    \"DiningTable\": \"A dining table is a wooden table with 4 legs and supports, usually found at the center of a kitchen or living room\",\n",
        "    \"DishSponge\": \"A dish sponge is asmall yellow rectangular item that is found near a kitchen sink for cleaning\",\n",
        "    \"DogBed\": \"A dog bed is circular elevated area found in living rooms or bedrooms in corners\",\n",
        "    \"Drawer\": \"A drawer is a wooden container that can be pulled horizontally to open and store things in that is usually found on the walls of kitchens or on dressers.\",\n",
        "    \"Dresser\": \"A dresser is a large wooden object that consists of many drawers for storage, usually found near the walls of a room.\",\n",
        "    \"Dumbbell\": \"A dumbbell is a metal stick with two black cylindrical weights on either side, usually found in corners of a room\",\n",
        "    \"Egg\": \"An egg is a white oval-shaped item that can be cracked and is usually found on kitchen counters\",\n",
        "    \"Faucet\": \"A faucet is a metal fixture on sinks in bathrooms or bedrooms where water comes from\",\n",
        "    \"Floor\": \"The floor is the structure below you in every room that supports you\",\n",
        "    \"FloorLamp\": \"A floor lamp is a tall light fixture that has a base on the floor to support it\",\n",
        "    \"Footstool\": \"A footstool is a small object that looks like a miniature table that can be stepped on to reach higher elevations\",\n",
        "    \"Fork\": \"A fork is a small metal utensil with 3 prongs at the end to hold onto food when eating on tables\",\n",
        "    \"Fridge\": \"A fridge is a large white structure next to walls in kitchens that holds food at cold temperatures\",\n",
        "    \"GarbageBag\": \"A garbage bag is a black flimsy object with wrinkles and an open top to place things in\",\n",
        "    \"GarbageCan\": \"A garbage can is a tall gray cylindrical object with an open top, usually found in the corners of rooms\",\n",
        "    \"HandTowel\": \"A hand towel is a small white square of cloth usually found on a holder near a sink\",\n",
        "    \"Kettle\": \"A kettle is a black spherical object with a handle at the top and a shaft protruding from it to pour liquid out of\",\n",
        "    \"KeyChain\": \"A key chain is a metallic circle usually found on tables with many metal or plastic keys or other objects dangling from it\",\n",
        "    \"Knife\": \"A knife is a small, thin, metal utensil usually found on dining tables or in kitchens; it has a sharp edge on one side\",\n",
        "    \"Ladle\": \"A ladle is a metal utensil with a partial sphere on one end to scoop items and a long handle on the other end to hold onto\",\n",
        "    \"Laptop\": \"A laptop is a rectangular metal object that can be opened to reveal a screen within, usually found on tables\",\n",
        "    \"Lettuce\": \"Lettuce is a small green spherical vegetable found on tables in kitchens\",\n",
        "    \"LightSwitch\": \"A light switch is a small rectangular area on walls next to doors with a switch in the middle\",\n",
        "    \"Microwave\": \"A microwave is a cubical metallic object with a clear front found on kitchen counters\",\n",
        "    \"Mirror\": \"A mirror is a fixture on walls or a handheld item that reflects the image that faces it\",\n",
        "    \"Mug\": \"A mug is a small cylindrical object with a handle at its side, usually placed on tables or filled with liquid\",\n",
        "    \"Newspaper\": \"A newspaper is a rectangular stack of paper with text written on it, usually found on tables\",\n",
        "    \"Ottoman\": \"An ottoman is a stool situated next to sofas or chairs made of soft materials\",\n",
        "    \"Painting\": \"A painting is an intricate two dimensional rectangle of colors placed on a wall\",\n",
        "    \"Pan\": \"A pan is a black circular object with small walls and a handle protruding from it, usually found on kitchen counters or stoves\",\n",
        "    \"PaperTowelRoll\": \"A paper towel roll is a metal fixture with a vertical roll of white paper wrapped around it, usually found on kitchen counters\",\n",
        "    \"Pen\": \"A pen is a long black stick with a sharp point at one end, usually found on tables\",\n",
        "    \"Pencil\": \"A pencil is a yellow wooden stick with a sharp point at one end, usually found on tables\",\n",
        "    \"PepperShaker\": \"A pepper shaker is a cylindrical object filled with black powder in the middle, topped with a metallic cap\",\n",
        "    \"Pillow\": \"A pillow is a soft white rectangle found at the front of beds\",\n",
        "    \"Plate\": \"A plate is a circular, smooth, white object with an incline at the ends, usually found on tables or kitchen counters\",\n",
        "    \"Plunger\": \"A plunger is a long stick with a half-circle attached to it at the end so that the half-circle is facing down\",\n",
        "    \"Poster\": \"A poster is a large picture of anything pasted onto a wall\",\n",
        "    \"Pot\": \"A pot is a black circular object with tall walls and a handle protruding from it, usually found on kitchen counters or stoves\",\n",
        "    \"Potato\": \"A potato is a spherical brown object with dots on it, usually found on kitchen counters\",\n",
        "    \"RemoteControl\": \"A remote control is a thin black rectangular object with assorted buttons on it, usually found on a table near a sofa\",\n",
        "    \"Safe\": \"A safe is a gray metal box usually placed on tables that can be opened with a lock password.\",\n",
        "    \"SaltShaker\": \"A salt shaker is a clear cylindrical container filled with white powder topped with a metal cap with holes.\",\n",
        "    \"ScrubBrush\": \"A scrub brush is a wooden shaft with a handle on one end and bristles on the other end to brush hair, and it is usually found on tables.\",\n",
        "    \"Shelf\": \"A wooden plank situated on a wall to place items on at an elevated height\",\n",
        "    \"ShelvingUnit\": \"A combination of multiple shelves to store items at different levels of elevation along walls\",\n",
        "    \"ShowerCurtain\": \"A vertical cloth that covers the front of a shower in bathrooms\",\n",
        "    \"ShowerDoor\": \"A clear door that covers showers in bathrooms to prevent water from falling out\",\n",
        "    \"ShowerGlass\": \"Shower glass covers the front of showers in a clear material in bathrooms\",\n",
        "    \"ShowerHead\": \"A shower head is a metal object in a shower where water comes from\",\n",
        "    \"SideTable\": \"A side table small table found next to sofas, beds, or chairs that is usually elevated and made of wood\",\n",
        "    \"Sink\": \"A sink is a cube-shaped object with a hole at the top found in kitchens and bathrooms with a faucet to wash hands and dishes\",\n",
        "    \"SoapBar\": \"A soap bar is a small white rectangle found in the showers of bathrooms that is used for bathing\",\n",
        "    \"SoapBottle\": \"A soap bottle is a cylindrical object found in showers that contains liquid for cleaning\",\n",
        "    \"Sofa\": \"A sofa is a large rectangular object made of soft materials in centers of living rooms for sitting on\",\n",
        "    \"Spatula\": \"A spatula is a metal utensil with a handle on one side and a widened flat part on the other side for picking up and lifting objects during cooking.\",\n",
        "    \"Spoon\": \"A spoon is a small  metal utensil with a handle on one side and a head with an inside curve to scoop up items while eating.\",\n",
        "    \"SprayBottle\": \"A spray bottle is a cylindrical translucent container with a hook-shaped head to spray liquids within the container, usually found near cabinets.\",\n",
        "    \"Statue\": \"A statue is an abstract 3d object made of stone usually found on tables that is used as a decorative item.\",\n",
        "    \"Stool\": \"A stool is a small table usually placed on the floor that people can stand on to increase height.\",\n",
        "    \"StoveBurner\": \"A stove burner is a large metal box with a black top situated along the walls of kitchens between counters.\",\n",
        "    \"StoveKnob\": \"A stove knob is a small circular metal item connected to a stove burner that can be turned.\",\n",
        "    \"TableTopDecor\": \"Table top decor is a small decorative item found on top of tables\",\n",
        "    \"TeddyBear\": \"A teddy bear is a brown, fluffy item in the shape of an animal usually found on beds or nightstands.\",\n",
        "    \"Television\": \"A television is a large, black, flat screen situated near walls that can be turned on to view different things.\",\n",
        "    \"TennisRacket\": \"A tennis racket is an ovular shape with intersecting lines of rope in the middle, as well as a handle on one end.\",\n",
        "    \"TissueBox\": \"A tissue box is a rectangular shape object with an opening at the top to pull tissues, or small white squares, out of.\",\n",
        "    \"Toaster\": \"A toaster is a metal cube-shaped objects with two to four slits at the top to insert bread in to heat it.\",\n",
        "    \"Toilet\": \"A toilet is a white ceramic object found in bathrooms that connects directly to the wall and has an ovular opening in the middle.\",\n",
        "    \"ToiletPaper\": \"Toilet paper consists of a roll of white paper situated near a toilet in bathrooms\",\n",
        "    \"ToiletPaperHanger\": \"A toilet paper hanger is a metal L-shaped item protruding from the wall of of bathroom near a toilet\",\n",
        "    \"Tomato\": \"A tomato is a red circular object with a green stem on top, usually found on kitchen counters\",\n",
        "    \"Towel\": \"A towel is a white rectangular object made of cloth that hangs from walls in bathrooms.\",\n",
        "    \"TowelHolder\": \"A towel holder is a metal bar hanging from the walls of a bathroom that holds a white towel.\",\n",
        "    \"TVStand\": \"A television stand is a black metal bracket under a television that holds it up from the floor.\",\n",
        "    \"VacuumCleaner\": \"A vacuum cleaner is a long machine with wheels under and a handle at the top and a dust container in the middle.\",\n",
        "    \"Vase\": \"A vase is an intricately shaped cylindrical object with an open top to place flowers or liquids within\",\n",
        "    \"Watch\": \"A small circular object with a white face surrounded by a a border and two metal bands coming off of it.\",\n",
        "    \"WateringCan\": \"A watering can is a plastic cylinder with a handle on top and a small pipe to pour water from.\",\n",
        "    \"Window\": \"A window is a clear rectangular space on a wall that can be seen through\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7IoQXiGxCNQ"
      },
      "outputs": [],
      "source": [
        "from ai2thor.controller import Controller\n",
        "import gym\n",
        "import torch\n",
        "import gym.spaces as spaces\n",
        "import random\n",
        "from stable_baselines3.common.policies import obs_as_tensor\n",
        "from collections import OrderedDict\n",
        "import cv2\n",
        "\n",
        "def predict_proba(model, state, lstm):\n",
        "    obs = obs_as_tensor(state.copy(), model.policy.device)\n",
        "    obs = obs.unsqueeze(0)\n",
        "    episode_starts = torch.ones(1)\n",
        "    # obs = obs.unsqueeze(2)\n",
        "    print(obs.size())\n",
        "    dis = model.policy.get_distribution(obs, lstm, episode_starts)\n",
        "    #probs = dis.distribution.probs\n",
        "    #probs_np = probs.detach().numpy()\n",
        "    return dis\n",
        "\n",
        "\n",
        "class AI2thor(gym.Env):\n",
        "    def __init__(self, ttv):\n",
        "        super(AI2thor, self).__init__()\n",
        "        \n",
        "        if ttv=='train':\n",
        "            self.rooms = trainMats\n",
        "        elif ttv=='val':\n",
        "            self.rooms = valMats\n",
        "        else:\n",
        "            self.rooms = testMats\n",
        "            \n",
        "        self.old_actions = [\"MoveAhead\", \"MoveBack\", \"MoveLeft\", \"MoveRight\", \"RotateLeft\", \"RotateRight\", \"LookUp\", \"LookDown\", \"Crouch\", \"Stand\", \"Done\"]\n",
        "        self.actions = [\"MoveAhead\", \"MoveBack\", \"MoveLeft\", \"MoveRight\", \"RotateLeft\", \"RotateRight\", \"Done\"]\n",
        "        self.action_space = spaces.Discrete(7)\n",
        "        \n",
        "        # self.observation_space = spaces.Dict({\n",
        "        #     \"image\": spaces.Box(low=0, high=255, shape=(448, 448, 3)),\n",
        "        #     \"text\": spaces.Box(low=-1.0, high=1.0, shape=(1,384))\n",
        "        # })\n",
        "        self.observation_space = spaces.Box(low=-1.0, high=255, shape=(2048+384, 1))\n",
        "        \n",
        "        self.text_processor = GeneratorPreprocess()\n",
        "            \n",
        "        self.ttv = ttv\n",
        "        self.controller = None\n",
        "        self.current_obj = None\n",
        "        self.obj_position = None\n",
        "        self.done = False\n",
        "        self.obj_vect = None\n",
        "        self.step_count = 0\n",
        "        self.total_r = 0;\n",
        "        self.total_rs = []\n",
        "        self.chosen_objects = []\n",
        "        self.num_objs = 0\n",
        "                \n",
        "    def step(self, action):\n",
        "        print(\"step: \", self.step_count)\n",
        "        print(\"action: \", self.actions[action])\n",
        "        event = self.controller.step(self.actions[action])\n",
        "        reward = 0\n",
        "        self.total_r+=reward\n",
        "        done = False\n",
        "        if self.actions[action]==\"Done\" or self.step_count>=32:\n",
        "                ag_pos = event.metadata[\"agent\"][\"position\"]\n",
        "                reward = 1-0.25*self.compute_distance(ag_pos)\n",
        "                if reward<0: reward=0\n",
        "                self.total_r+=reward\n",
        "                print(\"Finished! End reward: \",self.total_r)\n",
        "                self.total_rs.append(self.total_r)   \n",
        "                done = True\n",
        "                print(\"\\n\")\n",
        "        info = {\n",
        "            \"episode\": None,\n",
        "            \"is_success\": None\n",
        "        }\n",
        "        #probs = predict_proba(model, self.current_state)\n",
        "        #print(\"probs: \",probs)\n",
        "        cv2.imwrite(\"./im_{}.jpg\".format(self.step_count), cv2.cvtColor(event.cv2img, cv2.COLOR_RGB2BGR))\n",
        "        self.current_state = self.observation_space.sample()\n",
        "        #print(\"actionsuccess: \", event.metadata[\"lastActionSuccess\"])\n",
        "        print(\"error: \", event.metadata[\"errorMessage\"])\n",
        "        # self.current_state[\"image\"] = event.frame\n",
        "        # self.current_state[\"text\"] = self.obj_vect.numpy()\n",
        "        self.current_state = self.combine_inputs(event.frame.astype(np.uint8), self.obj_vect.detach().numpy())\n",
        "        self.step_count+=1;\n",
        "\n",
        "        return self.current_state, reward, done, info\n",
        "        \n",
        "\n",
        "    def reset(self):\n",
        "        self.step_count = 0\n",
        "        self.total_r = 0\n",
        "        self.controller = Controller(height=448, width=448, gridSize=0.25, snapToGrid=False)\n",
        "        self.current_obj = None\n",
        "        self.done = False\n",
        "        self.current_state = self.observation_space.sample()\n",
        "\n",
        "        #if self.num_objs>=3 or self.num_objs==0:\n",
        "        self.num_objs = 0\n",
        "        self.chosen_objects.clear()\n",
        "        self.choose_scene()\n",
        "\n",
        "        event, obj_vect = self.domain_rand()\n",
        "        # self.current_state[\"image\"] = event.frame\n",
        "        # self.current_state[\"text\"] = obj_vect.numpy()\n",
        "        self.current_state = self.combine_inputs(event.frame.astype(np.uint8), obj_vect.detach().numpy())\n",
        "        print(\"reset\")\n",
        "        self.num_objs += 1\n",
        "        return self.current_state\n",
        "\n",
        "    def combine_inputs(self, frame, obj_vect):\n",
        "        ar = np.zeros(2048+384)\n",
        "        ar[:-384] = execute_resnet(frame).numpy()\n",
        "        ar[2048:2048+384] = obj_vect\n",
        "        return np.expand_dims(ar, axis=1)\n",
        "        \n",
        "    def choose_scene(self):\n",
        "        roomID = self.rooms[np.random.randint(0, len(self.rooms))]\n",
        "        self.controller.reset(scene=roomID)\n",
        "\n",
        "    def reset_current_scene(self):\n",
        "        metadata = self.controller.last_event.metadata[\"sceneBounds\"]\n",
        "        x = set()\n",
        "        y = set()\n",
        "        for m in metadata:\n",
        "          x.add(m[0])\n",
        "          y.add(m[1])\n",
        "        random_x = random.uniform(x[0], x[1])\n",
        "        random_y = random.uniform(y[0], y[1])\n",
        "        self.controller.step(action=\"Teleport\", position=dict(random_x, random_y, 0), standing=True)\n",
        "\n",
        "\n",
        "    def compute_distance(self, ag_pos):\n",
        "        x = abs(ag_pos['x']-self.obj_position['x'])\n",
        "        z = abs(ag_pos['z']-self.obj_position['z'])\n",
        "        return np.sqrt(x**2+z**2)\n",
        "        \n",
        "    def domain_rand(self):\n",
        "        if self.ttv=='train':\n",
        "            event = self.controller.step(\n",
        "                action=\"RandomizeMaterials\",\n",
        "                useTrainMaterials=True,\n",
        "                useValMaterials=False,\n",
        "                useTestMaterials=False\n",
        "            )\n",
        "            obj_vect = self.choose_object(event)\n",
        "            return event, obj_vect\n",
        "        elif self.ttv=='val':\n",
        "            event = self.controller.step(\n",
        "                action=\"RandomizeMaterials\",\n",
        "                useTrainMaterials=False,\n",
        "                useValMaterials=True,\n",
        "                useTestMaterials=False\n",
        "            )\n",
        "            obj_vect = self.choose_object(event)\n",
        "            return event, obj_vect\n",
        "        elif self.ttv=='test':\n",
        "            event = self.controller.step(\n",
        "                action=\"RandomizeMaterials\",\n",
        "                useTrainMaterials=False,\n",
        "                useValMaterials=False,\n",
        "                useTestMaterials=True\n",
        "            )\n",
        "            obj_vect = self.choose_object(event)\n",
        "            return event, obj_vect\n",
        "        \n",
        "    def choose_object(self, event):\n",
        "        total_objects = len(event.metadata[\"objects\"])\n",
        "        while True:\n",
        "          ix = np.random.randint(0, total_objects)\n",
        "          self.current_obj = event.metadata[\"objects\"][ix]\n",
        "          self.obj_position = self.current_obj[\"position\"]\n",
        "          if self.current_obj[\"objectType\"] in objects and self.current_obj[\"objectType\"] not in self.chosen_objects:\n",
        "            break;\n",
        "        self.chosen_objects.append(self.current_obj[\"objectType\"])\n",
        "        self.obj_vect = self.text_processor(objects[self.current_obj[\"objectType\"]])\n",
        "        return self.obj_vect\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate policy own helper function bc stable baselines goofy ahhhhhhhhhhh\n",
        "\n",
        "def evaluate(env, model):\n",
        "    episode_rewards = []\n",
        "    lstm_state = []\n",
        "    \n",
        "    for _ in range(10):\n",
        "      obs = env.reset()\n",
        "      step_rewards = []\n",
        "      \n",
        "      while True:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        #print(\"states: \", _states[0].shape, _states[1].shape)\n",
        "        print(\"action:\", action)\n",
        "        print(\"proba: \", predict_proba(model, obs, _states)[0].distribution.probs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        step_rewards.append(reward)\n",
        "        lstm_state = _states\n",
        "        if done:\n",
        "          break\n",
        "      episode_rewards.append(sum(step_rewards))\n",
        "    return episode_rewards"
      ],
      "metadata": {
        "id": "CK2mb0wakCeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMYcRJD_xCSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "6333a34736944652817ba8f5b9a5c4fe",
            "8cefe608b1044fc48eaaad47ef72de22",
            "eebfbd02a67145ed9c5fab45be8c4876",
            "1e3376c712414b218e88c47f093db9af",
            "047c5995702641debc41cb2c2d229765",
            "e9ac154f788c4a238a542fb3f865f20a",
            "8bd79606f51a402bb2d87f4c06789ef8",
            "8d215cbee30a4e5fbbd51baae6c88cdb",
            "c28f7b6d85a54b3d8830f189e595b959",
            "b2527c5dbf234cb3b578142a1354d8fb",
            "ca1ce7c1f7dc4599b4b2e70a79ffa92e",
            "42a9c39fda214e63a858be7a40e61cd7",
            "1b0537d56f9f41519250e9c59ddf4455",
            "840e351f8d244e48af22be7b143d602f",
            "c963ef9b6b7946b3987c403b506ea6c0",
            "4a63bb37652a41fc8397aef93a052a8e",
            "b8de9cf4218342469d4f3e455560f9e6",
            "0f29facbf8414e7bb2de3c3fd7fd32ce",
            "704fc8d6d6a142f8812bb3c57d444545",
            "7cc3561cc4c34be5ac75ee262ccd10f7",
            "990d6e97319941a7a6cb179a9f4f4b02",
            "bfeada02b593485d9ab406912737c2fd",
            "ac144aa3357d4f2eb6fa0fec4bdd1bfb",
            "de1c8e95db7c44bc871f4bf7994e8dd8",
            "7dba15f566d94bda8b89b22773462499",
            "9c652c4de86f4bc6abf5a06ee118266c",
            "8ea5f221f7e6450cb0424de98569aa5e",
            "56ed6d3bd8424f0483b183c4c5846728",
            "5696a35014ac472592b823d0f9718fb3",
            "375f0944c08d4248ae5028ebfd89f0d3",
            "0ecb0263d62548c1b883549bc0605c87",
            "0a9fa4d8a194402baeb2d3199d90cfcc",
            "6cb758f3f5484b4ea5b1333d8a87cf55",
            "33c4fca029474618bec06b96db3e4d80",
            "dfd079029b1946f6a2a33089fc11cba5",
            "e6b7c04d791e4de78c3d6fe71a9bf230",
            "84a16413d17e42089668a1271e708241",
            "326002d44cad426a83ee43e4dc835d8f",
            "962a3a6ab2d445e3a917372b9c5b4f3e",
            "889ea38a2b9849daa0723ef42306aa04",
            "6dba7d87212043c3abb6bf63abd991bf",
            "e01bc44800fb4630979f626170ff5002",
            "e64d221b66bb43b4a395e82c4b2bf72e",
            "2f42047b9bc94af4a3ef6e91b6d6f2e2",
            "404a195a8238425c8e475548a5560234",
            "4320a88cb9c144109f606a7ab816b2eb",
            "d9ca622d71084e41876411616da4d8cd",
            "646dc818feff4bf185e0d0a6510d175b",
            "ee8067c2b01342ef9cfa54c99556c77f",
            "7eddcf3053e3495d977d50589978b5a9",
            "a3d070a0da7e43e5866c79398ba366b5",
            "56e8058f65ed4deb80076c67fcf60652",
            "d2fca1a910924c95b9a4e03dce42a117",
            "70257162561b42b39cb8095852f50cab",
            "32911d299d274945acc99dad7288ec94",
            "caabaa957a384c49a1a31c88fa5ff2cc",
            "dd7f14bb55de499e97d5a31be4e45255",
            "79fe135890ca4e308fdc7e333c8d40a2",
            "68b0c91fd7374fc68d232da46041dd54",
            "644e5ed463f84910ae307a38b7f122a3",
            "f2e74f56d995422d970eb2a7937a630d",
            "c7e92e04af894813bfb001d9917efa4a",
            "6edbfde152fc4c8dba532589028a3bf4",
            "278cf7a024594390851af5750523d9a6",
            "faec5998971147169b7163488b8c0295",
            "bf5d4819e4984d25a8983a3d2a431054"
          ]
        },
        "outputId": "17a80779-2bbb-43d7-befa-0314bfb9cd82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6333a34736944652817ba8f5b9a5c4fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42a9c39fda214e63a858be7a40e61cd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac144aa3357d4f2eb6fa0fec4bdd1bfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c4fca029474618bec06b96db3e4d80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "404a195a8238425c8e475548a5560234"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caabaa957a384c49a1a31c88fa5ff2cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO, A2C, DDPG, DQN\n",
        "from sb3_contrib import RecurrentPPO\n",
        "import numpy as np\n",
        "\n",
        "env = AI2thor('train')\n",
        "print(env.num_objs)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class = FeatureExtractor\n",
        ")\n",
        "model = RecurrentPPO(policy=\"MlpLstmPolicy\", \n",
        "            env=env, \n",
        "            learning_rate=0.001, \n",
        "            verbose=1, \n",
        "            device=device, \n",
        "            n_steps=32,\n",
        "            batch_size=32,\n",
        "            ent_coef=0.35,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            normalize_advantage=True\n",
        "          )\n",
        "# 0.1425+0.1428+0.1426+0.1431+0.1430+0.143+0.1429"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load('./model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc7kirnmxKRd",
        "outputId": "efd8ae1e-6459-4901-d8fa-0ca314aa717d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO at 0x7fe5502aa520>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDAvd6b0xCWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fc029a-5892-48d7-8a57-696ba4511eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  1\n",
            "action:  Done\n",
            "Finished! End reward:  0.36246091584132645\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  Done\n",
            "Finished! End reward:  0.16049924451374287\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  5\n",
            "action:  Done\n",
            "Finished! End reward:  0.5708324195345602\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.6365422001453787\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  Done\n",
            "Finished! End reward:  0.24927240349118795\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  Done\n",
            "Finished! End reward:  0.5702836683835161\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  Done\n",
            "Finished! End reward:  0.6202697797777483\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  7\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  8\n",
            "action:  Done\n",
            "Finished! End reward:  0.555379285936928\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.88     |\n",
            "|    ep_rew_mean     | 0.466    |\n",
            "| time/              |          |\n",
            "|    fps             | 0        |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 243      |\n",
            "|    total_timesteps | 32       |\n",
            "---------------------------------\n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  Done\n",
            "Finished! End reward:  0\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  CoffeeTable_efe4814d is blocking Agent 0 from moving by (0.2500, 0.0000, 0.0000).\n",
            "step:  1\n",
            "action:  Done\n",
            "Finished! End reward:  0.721014515735134\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  5\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  7\n",
            "action:  Done\n",
            "Finished! End reward:  0.29983544741342216\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  Done\n",
            "Finished! End reward:  0.4320619085430861\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  Scene bounds do not contain target position: (-1.0, -6.7, 0.3)\n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  Scene bounds do not contain target position: (-0.8, -6.7, 0.0)\n",
            "step:  2\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  Done\n",
            "Finished! End reward:  0.6713535824322163\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.20615455105331448\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveBack\n",
            "error:  \n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 4.07      |\n",
            "|    ep_rew_mean          | 0.433     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 0         |\n",
            "|    iterations           | 2         |\n",
            "|    time_elapsed         | 434       |\n",
            "|    total_timesteps      | 64        |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0322418 |\n",
            "|    clip_fraction        | 0.178     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.94     |\n",
            "|    explained_variance   | 0.0757    |\n",
            "|    learning_rate        | 0.001     |\n",
            "|    loss                 | -0.713    |\n",
            "|    n_updates            | 10        |\n",
            "|    policy_gradient_loss | -0.0275   |\n",
            "|    value_loss           | 0.968     |\n",
            "---------------------------------------\n",
            "step:  7\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  8\n",
            "action:  Done\n",
            "Finished! End reward:  0.300905918567834\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  Done\n",
            "Finished! End reward:  0.27600694300509576\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  2\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.4450174052167074\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  Done\n",
            "Finished! End reward:  0.5910786651290177\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  Ottoman_b96ecbf4 is blocking Agent 0 from moving by (0.0000, 0.0000, 0.2500).\n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  4\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  7\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  8\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  9\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  10\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  11\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  12\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  13\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  14\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  15\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  16\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  17\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  18\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  19\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  20\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  21\n",
            "action:  Done\n",
            "Finished! End reward:  0.05307466878104217\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5           |\n",
            "|    ep_rew_mean          | 0.406       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 0           |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 545         |\n",
            "|    total_timesteps      | 96          |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016505523 |\n",
            "|    clip_fraction        | 0.0938      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.92       |\n",
            "|    explained_variance   | -0.232      |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | -0.66       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    value_loss           | 0.103       |\n",
            "-----------------------------------------\n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  7\n",
            "action:  Done\n",
            "Finished! End reward:  0.32651861716043096\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  Done\n",
            "Finished! End reward:  0.49716674159875507\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  Done\n",
            "Finished! End reward:  0.48635333442822803\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  Done\n",
            "Finished! End reward:  0.32509155824207503\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  5\n",
            "action:  Done\n",
            "Finished! End reward:  0\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveBack\n",
            "error:  \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.12        |\n",
            "|    ep_rew_mean          | 0.39        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 0           |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 755         |\n",
            "|    total_timesteps      | 128         |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024253923 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.92       |\n",
            "|    explained_variance   | 0.0408      |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | -0.722      |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0363     |\n",
            "|    value_loss           | 0.0228      |\n",
            "-----------------------------------------\n",
            "step:  5\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  6\n",
            "action:  Done\n",
            "Finished! End reward:  0.0185024898159456\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  Done\n",
            "Finished! End reward:  0.07154107313546965\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  7\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  8\n",
            "action:  Done\n",
            "Finished! End reward:  0.5479198762834082\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  4\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  7\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  8\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  9\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  10\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  11\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  12\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  13\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  14\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  15\n",
            "action:  Done\n",
            "Finished! End reward:  0.2364242469851615\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 5.64       |\n",
            "|    ep_rew_mean          | 0.365      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 0          |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 901        |\n",
            "|    total_timesteps      | 160        |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03456229 |\n",
            "|    clip_fraction        | 0.225      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.93      |\n",
            "|    explained_variance   | -0.067     |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | -0.723     |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0334    |\n",
            "|    value_loss           | 0.0294     |\n",
            "----------------------------------------\n",
            "step:  2\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  7\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  8\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  9\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  10\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  11\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  12\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  13\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  14\n",
            "action:  Done\n",
            "Finished! End reward:  0.3897200679614712\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  7\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  8\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  9\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  10\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  11\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  12\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  13\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  14\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  15\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  16\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  17\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  18\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 5.97        |\n",
            "|    ep_rew_mean          | 0.366       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 0           |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 961         |\n",
            "|    total_timesteps      | 192         |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014264127 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.94       |\n",
            "|    explained_variance   | -0.101      |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | -0.699      |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    value_loss           | 0.0232      |\n",
            "-----------------------------------------\n",
            "step:  19\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  20\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  21\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  22\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  23\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  24\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  25\n",
            "action:  Done\n",
            "Finished! End reward:  0\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  FP405:polySurface387 is blocking Agent 0 from moving by (0.0000, 0.0000, -0.2500).\n",
            "step:  3\n",
            "action:  MoveLeft\n",
            "error:  FP405:polySurface387 is blocking Agent 0 from moving by (0.0000, 0.0000, -0.2500).\n",
            "step:  4\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  5\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  7\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  8\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  9\n",
            "action:  Done\n",
            "Finished! End reward:  0.5008388183301852\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  Done\n",
            "Finished! End reward:  0.6648516910824025\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  Done\n",
            "Finished! End reward:  0.14695765058988797\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.6977258025247426\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  Done\n",
            "Finished! End reward:  0\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 6.31        |\n",
            "|    ep_rew_mean          | 0.361       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 0           |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 1149        |\n",
            "|    total_timesteps      | 224         |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032874696 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.92       |\n",
            "|    explained_variance   | -0.0581     |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | -0.743      |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0432     |\n",
            "|    value_loss           | 0.00388     |\n",
            "-----------------------------------------\n",
            "step:  3\n",
            "action:  Done\n",
            "Finished! End reward:  0.46599976591765435\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.5870169931432672\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        }
      ],
      "source": [
        "model.learn(250, reset_num_timesteps=False)\n",
        "model.save(\"./modelnew.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjFwa5iadgpF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e88e68d3-94c7-4acf-f44d-49f683c80368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveLeft\n",
            "error:  FP326:StandardDoor1.019 is blocking Agent 0 from moving by (0.0000, 0.0000, 0.2500).\n",
            "step:  2\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  3\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  Done\n",
            "Finished! End reward:  0.10501786539978997\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  Blinds_7d49b6d0 is blocking Agent 0 from moving by (0.0000, 0.0000, 0.2500).\n",
            "step:  1\n",
            "action:  RotateLeft\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  3\n",
            "action:  Done\n",
            "Finished! End reward:  0.3153036770599006\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  2\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  3\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  4\n",
            "action:  MoveRight\n",
            "error:  \n",
            "step:  5\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  6\n",
            "action:  MoveRight\n",
            "error:  StoveBase2 is blocking Agent 0 from moving by (0.2500, 0.0000, 0.0000).\n",
            "step:  7\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  8\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  9\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  10\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  11\n",
            "action:  MoveAhead\n",
            "error:  \n",
            "step:  12\n",
            "action:  MoveLeft\n",
            "error:  OVENDOORBOTTOM.001 is blocking Agent 0 from moving by (0.2500, 0.0000, 0.0000).\n",
            "step:  13\n",
            "action:  MoveBack\n",
            "error:  \n",
            "step:  14\n",
            "action:  RotateRight\n",
            "error:  \n",
            "step:  15\n",
            "action:  Done\n",
            "Finished! End reward:  0.5810322329967559\n",
            "\n",
            "\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reset\n",
            "step:  0\n",
            "action:  MoveLeft\n",
            "error:  \n",
            "step:  1\n",
            "action:  MoveBack\n",
            "error:  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-69a523e5a231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode_counts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisode_count_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_starts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mcurrent_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-10-30e291113695>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# self.current_state[\"image\"] = event.frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# self.current_state[\"text\"] = self.obj_vect.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-30e291113695>\u001b[0m in \u001b[0;36mcombine_inputs\u001b[0;34m(self, frame, obj_vect)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcombine_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_vect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f26f11282c9c>\u001b[0m in \u001b[0;36mexecute_resnet\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImg2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# preprocess = transforms.Compose([\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m#     transforms.RandomResizedCrop(224),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/img2vec_pytorch/img_to_vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cuda, model, layer, layer_output_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_and_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/img2vec_pytorch/img_to_vec.py\u001b[0m in \u001b[0;36m_get_model_and_layer\u001b[0;34m(self, model_name, layer)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avgpool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_only_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36minner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_weights_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mresnet50\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[0;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_classes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fan_out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroupNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_normal_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "n_eval_episodes = 5\n",
        "\n",
        "outputs = []\n",
        "\n",
        "\n",
        "model.load('./modelnew.pt')\n",
        "\n",
        "test_env = AI2thor('test')\n",
        "\n",
        "for x in range(1):\n",
        "  print(x+1)\n",
        "  x = evaluate_policy(model, test_env, n_eval_episodes=5, deterministic=False)\n",
        "  print(\"complete\")\n",
        "  outputs.append(x)\n",
        "\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LENH8a_c76se",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "35388fa1-a402-4669-92ce-4c6ac6e5b572"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9a17be8e4448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAI2thor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.load(\"./modelnew.pt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AI2thor' is not defined"
          ]
        }
      ],
      "source": [
        "test_env = AI2thor('test')\n",
        "\n",
        "#model.load(\"./modelnew.pt\")\n",
        "\n",
        "for x in range(1):\n",
        "  print(x+1)\n",
        "  x = evaluate(test_env, model)\n",
        "  print(\"complete\")\n",
        "  outputs.append(x)\n",
        "\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpTBEAlTiRNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6333a34736944652817ba8f5b9a5c4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cefe608b1044fc48eaaad47ef72de22",
              "IPY_MODEL_eebfbd02a67145ed9c5fab45be8c4876",
              "IPY_MODEL_1e3376c712414b218e88c47f093db9af"
            ],
            "layout": "IPY_MODEL_047c5995702641debc41cb2c2d229765"
          }
        },
        "8cefe608b1044fc48eaaad47ef72de22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ac154f788c4a238a542fb3f865f20a",
            "placeholder": "​",
            "style": "IPY_MODEL_8bd79606f51a402bb2d87f4c06789ef8",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "eebfbd02a67145ed9c5fab45be8c4876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d215cbee30a4e5fbbd51baae6c88cdb",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28f7b6d85a54b3d8830f189e595b959",
            "value": 350
          }
        },
        "1e3376c712414b218e88c47f093db9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2527c5dbf234cb3b578142a1354d8fb",
            "placeholder": "​",
            "style": "IPY_MODEL_ca1ce7c1f7dc4599b4b2e70a79ffa92e",
            "value": " 350/350 [00:00&lt;00:00, 22.7kB/s]"
          }
        },
        "047c5995702641debc41cb2c2d229765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ac154f788c4a238a542fb3f865f20a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd79606f51a402bb2d87f4c06789ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d215cbee30a4e5fbbd51baae6c88cdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28f7b6d85a54b3d8830f189e595b959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2527c5dbf234cb3b578142a1354d8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca1ce7c1f7dc4599b4b2e70a79ffa92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42a9c39fda214e63a858be7a40e61cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b0537d56f9f41519250e9c59ddf4455",
              "IPY_MODEL_840e351f8d244e48af22be7b143d602f",
              "IPY_MODEL_c963ef9b6b7946b3987c403b506ea6c0"
            ],
            "layout": "IPY_MODEL_4a63bb37652a41fc8397aef93a052a8e"
          }
        },
        "1b0537d56f9f41519250e9c59ddf4455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8de9cf4218342469d4f3e455560f9e6",
            "placeholder": "​",
            "style": "IPY_MODEL_0f29facbf8414e7bb2de3c3fd7fd32ce",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "840e351f8d244e48af22be7b143d602f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704fc8d6d6a142f8812bb3c57d444545",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cc3561cc4c34be5ac75ee262ccd10f7",
            "value": 231508
          }
        },
        "c963ef9b6b7946b3987c403b506ea6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990d6e97319941a7a6cb179a9f4f4b02",
            "placeholder": "​",
            "style": "IPY_MODEL_bfeada02b593485d9ab406912737c2fd",
            "value": " 232k/232k [00:00&lt;00:00, 841kB/s]"
          }
        },
        "4a63bb37652a41fc8397aef93a052a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8de9cf4218342469d4f3e455560f9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f29facbf8414e7bb2de3c3fd7fd32ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "704fc8d6d6a142f8812bb3c57d444545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc3561cc4c34be5ac75ee262ccd10f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "990d6e97319941a7a6cb179a9f4f4b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfeada02b593485d9ab406912737c2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac144aa3357d4f2eb6fa0fec4bdd1bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de1c8e95db7c44bc871f4bf7994e8dd8",
              "IPY_MODEL_7dba15f566d94bda8b89b22773462499",
              "IPY_MODEL_9c652c4de86f4bc6abf5a06ee118266c"
            ],
            "layout": "IPY_MODEL_8ea5f221f7e6450cb0424de98569aa5e"
          }
        },
        "de1c8e95db7c44bc871f4bf7994e8dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ed6d3bd8424f0483b183c4c5846728",
            "placeholder": "​",
            "style": "IPY_MODEL_5696a35014ac472592b823d0f9718fb3",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "7dba15f566d94bda8b89b22773462499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375f0944c08d4248ae5028ebfd89f0d3",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ecb0263d62548c1b883549bc0605c87",
            "value": 466247
          }
        },
        "9c652c4de86f4bc6abf5a06ee118266c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a9fa4d8a194402baeb2d3199d90cfcc",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb758f3f5484b4ea5b1333d8a87cf55",
            "value": " 466k/466k [00:00&lt;00:00, 1.09MB/s]"
          }
        },
        "8ea5f221f7e6450cb0424de98569aa5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ed6d3bd8424f0483b183c4c5846728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5696a35014ac472592b823d0f9718fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "375f0944c08d4248ae5028ebfd89f0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecb0263d62548c1b883549bc0605c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a9fa4d8a194402baeb2d3199d90cfcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb758f3f5484b4ea5b1333d8a87cf55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c4fca029474618bec06b96db3e4d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfd079029b1946f6a2a33089fc11cba5",
              "IPY_MODEL_e6b7c04d791e4de78c3d6fe71a9bf230",
              "IPY_MODEL_84a16413d17e42089668a1271e708241"
            ],
            "layout": "IPY_MODEL_326002d44cad426a83ee43e4dc835d8f"
          }
        },
        "dfd079029b1946f6a2a33089fc11cba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962a3a6ab2d445e3a917372b9c5b4f3e",
            "placeholder": "​",
            "style": "IPY_MODEL_889ea38a2b9849daa0723ef42306aa04",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "e6b7c04d791e4de78c3d6fe71a9bf230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dba7d87212043c3abb6bf63abd991bf",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e01bc44800fb4630979f626170ff5002",
            "value": 112
          }
        },
        "84a16413d17e42089668a1271e708241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e64d221b66bb43b4a395e82c4b2bf72e",
            "placeholder": "​",
            "style": "IPY_MODEL_2f42047b9bc94af4a3ef6e91b6d6f2e2",
            "value": " 112/112 [00:00&lt;00:00, 5.27kB/s]"
          }
        },
        "326002d44cad426a83ee43e4dc835d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962a3a6ab2d445e3a917372b9c5b4f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889ea38a2b9849daa0723ef42306aa04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dba7d87212043c3abb6bf63abd991bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01bc44800fb4630979f626170ff5002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e64d221b66bb43b4a395e82c4b2bf72e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f42047b9bc94af4a3ef6e91b6d6f2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "404a195a8238425c8e475548a5560234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4320a88cb9c144109f606a7ab816b2eb",
              "IPY_MODEL_d9ca622d71084e41876411616da4d8cd",
              "IPY_MODEL_646dc818feff4bf185e0d0a6510d175b"
            ],
            "layout": "IPY_MODEL_ee8067c2b01342ef9cfa54c99556c77f"
          }
        },
        "4320a88cb9c144109f606a7ab816b2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eddcf3053e3495d977d50589978b5a9",
            "placeholder": "​",
            "style": "IPY_MODEL_a3d070a0da7e43e5866c79398ba366b5",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d9ca622d71084e41876411616da4d8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e8058f65ed4deb80076c67fcf60652",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2fca1a910924c95b9a4e03dce42a117",
            "value": 612
          }
        },
        "646dc818feff4bf185e0d0a6510d175b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70257162561b42b39cb8095852f50cab",
            "placeholder": "​",
            "style": "IPY_MODEL_32911d299d274945acc99dad7288ec94",
            "value": " 612/612 [00:00&lt;00:00, 29.0kB/s]"
          }
        },
        "ee8067c2b01342ef9cfa54c99556c77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eddcf3053e3495d977d50589978b5a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d070a0da7e43e5866c79398ba366b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56e8058f65ed4deb80076c67fcf60652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2fca1a910924c95b9a4e03dce42a117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70257162561b42b39cb8095852f50cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32911d299d274945acc99dad7288ec94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caabaa957a384c49a1a31c88fa5ff2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd7f14bb55de499e97d5a31be4e45255",
              "IPY_MODEL_79fe135890ca4e308fdc7e333c8d40a2",
              "IPY_MODEL_68b0c91fd7374fc68d232da46041dd54"
            ],
            "layout": "IPY_MODEL_644e5ed463f84910ae307a38b7f122a3"
          }
        },
        "dd7f14bb55de499e97d5a31be4e45255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e74f56d995422d970eb2a7937a630d",
            "placeholder": "​",
            "style": "IPY_MODEL_c7e92e04af894813bfb001d9917efa4a",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "79fe135890ca4e308fdc7e333c8d40a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6edbfde152fc4c8dba532589028a3bf4",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_278cf7a024594390851af5750523d9a6",
            "value": 90888945
          }
        },
        "68b0c91fd7374fc68d232da46041dd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faec5998971147169b7163488b8c0295",
            "placeholder": "​",
            "style": "IPY_MODEL_bf5d4819e4984d25a8983a3d2a431054",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 403MB/s]"
          }
        },
        "644e5ed463f84910ae307a38b7f122a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e74f56d995422d970eb2a7937a630d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e92e04af894813bfb001d9917efa4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6edbfde152fc4c8dba532589028a3bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278cf7a024594390851af5750523d9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faec5998971147169b7163488b8c0295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5d4819e4984d25a8983a3d2a431054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}